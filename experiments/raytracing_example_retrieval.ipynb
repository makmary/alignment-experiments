{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9af8",
   "metadata": {},
   "source": [
    "## FIFTH EXPERIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1c301",
   "metadata": {},
   "source": [
    "В пятом эксперименте мы используем уже улучешенные 2d keypoints, которые мы получили при помощи аппарата PixSfM. Для данных 2d keypoints нам известные tentative matching graphs (трэки). При помощи ray tracing мы находим соответствия между 2д и 3д точками. Теперь у нас есть трэки для соответсвующих 3д точек. При помощи различных методов по разреживанию графов мы избавляемся от outliers и находим среднюю точку для данного трека.\n",
    "\n",
    "\n",
    "Пытаюсь построить кастомную геометричекскую верификацию мэтчей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d3767",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2f14e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ipython-sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b681f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext sql\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "\n",
    "import copy \n",
    "from collections import defaultdict\n",
    "import pycolmap\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/workspace/sk3d/dev.sk_robot_rgbd_data/src')\n",
    "sys.path.append(\"/workspace/pixel-perfect-sfm/\")\n",
    "sys.path.append(\"/workspace/pixel-perfect-sfm/Hierarchical-Localization\")\n",
    "\n",
    "from hloc import extract_features, match_features, match_dense, reconstruction, pairs_from_exhaustive, \\\n",
    "        visualization, pairs_from_retrieval\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils.viz_3d import init_figure, plot_points, plot_reconstruction, plot_camera_colmap\n",
    "from hloc.utils.read_write_model import  write_next_bytes, Point3D, Image, read_images_text, \\\n",
    "        read_points3D_binary, write_points3D_binary, write_images_binary, read_images_binary, \\\n",
    "        read_cameras_text\n",
    "\n",
    "from pixsfm import ostream_redirect\n",
    "\n",
    "from skrgbd.data.io import imgio\n",
    "from skrgbd.calibration.camera_models import load_colmap_camera\n",
    "from skrgbd.data.io.poses import load_poses\n",
    "\n",
    "from skrgbd.data.processing.depth_utils.occluded_mesh_rendering import MeshRenderer\n",
    "\n",
    "\n",
    "from utils import get_conf_upper_bound, draw_geometries\n",
    "\n",
    "# redirect the C++ outputs to notebook cells\n",
    "cpp_out = ostream_redirect(stderr=True, stdout=True)\n",
    "cpp_out.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561c9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"all\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ed1f2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689213b8",
   "metadata": {},
   "source": [
    "В **scene_name** необходимо задать имя объекта, над которым вы хотите провести эксперимент.\n",
    "\n",
    "**delete_previous_output** - если True, то удаляет все предыдущие файлы в папке outputs. Использовать супер осторожно.\n",
    "\n",
    "**has_cache** - если True, то у Вас уже существует файл с feature maps и он сохранен в папке cache_init. Это файл с feature maps Вы получаете только тогда, когда вы уже сделали featuremetric KA или BA для одного из ваших экспериментов.\n",
    "\n",
    "**show_visualization** - если True, то показывает визуализацию результата эксперимента (3d pointcloud, задектированные keypoints (features) и final reprojections для какого-то изображения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00259533",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_name = 'dragon'\n",
    "delete_previous_output = True\n",
    "\n",
    "has_cache = True\n",
    "show_visualization = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95297f",
   "metadata": {},
   "source": [
    "**poses** - путь к файлу images.txt с известными позами камер (каждая вторая строка пустая)\n",
    "\n",
    "**cameras_model** - путь к файлу cameras.txt, где хранятся GT параметры камеры\n",
    "\n",
    "**images** - путь к папке с изображениями для реконструкции\n",
    "\n",
    "**outputs** - путь к папке со всеми результатами\n",
    "\n",
    "**cache_init** - путь к кэш-файлу, его мы получаем во время того, когда делаем KA или BA. В этот файле хранятся featuremaps после  dense feature extraction. В среднем на одну картинку размером 2368х1952 уходит 3 минуты. Этот файл вообще нельзя трогать, поэтому мы копируем его в папку outputs для своего эксперимента и продолжаем работу.\n",
    "\n",
    "**cache_path** - тот же файл, что cache_init, с которым мы теперь будем работать во время эксперимента.\n",
    "\n",
    "**sfm_pairs** - файл с названиями пар изображений на каждой строке\n",
    "\n",
    "**features** - файл с features для каждой картинки, извлеченными при помощи feature_conf\n",
    "\n",
    "**refined_keypoints** - файл с улучшенными 2d keypoints, файл был получен после процедуры KA (Ketpoint Adjustment)\n",
    "\n",
    "**matches** - файл с matches для каждой пары картинок, извлеченными при помощи matcher_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "508281f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.h5  s2dnet_featuremaps_sparse.h5\r\n"
     ]
    }
   ],
   "source": [
    "root =  Path('/workspace')\n",
    "\n",
    "#poses\n",
    "poses = root / f'datasets/sk3d/dataset/{scene_name}/tis_right/rgb/images.txt'\n",
    "\n",
    "#cam_model\n",
    "camera_model = root / 'datasets/sk3d/dataset/calibration/tis_right/rgb/cameras.txt'\n",
    "\n",
    "images = root / f'datasets/sk3d/dataset/{scene_name}/tis_right/rgb/undistorted/ambient@best'\n",
    "outputs = root / f'pixel-perfect-sfm/outputs/{scene_name}/experiment_5_retrieval'\n",
    "\n",
    "if delete_previous_output:\n",
    "    !rm -rf $outputs \n",
    "    \n",
    "outputs.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "if has_cache:\n",
    "    cache_init = root / f'pixel-perfect-sfm/outputs/caches/{scene_name}/s2dnet_featuremaps_sparse.h5'\n",
    "    !cp -r $cache_init $outputs\n",
    "    cache_path = outputs / 's2dnet_featuremaps_sparse.h5'  \n",
    "\n",
    "    \n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'      # top-k most covisible in SIFT model\n",
    "loc_pairs = outputs / f'pairs-query-netvlad.txt'  # top-k retrieved by NetVLAD\n",
    "\n",
    "\n",
    "features = outputs / 'features.h5'\n",
    "refined_keypoints = outputs / 'refined_keypoints.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "\n",
    "# refined cameras\n",
    "\n",
    "!cp -r /workspace/pixel-perfect-sfm/outputs/dragon/experiment_2/features.h5 $outputs\n",
    "# !cp -r /workspace/pixel-perfect-sfm/outputs/dragon/experiment_2/refined/exp_not_refine_cam_params/refined_keypoints.h5 $outputs\n",
    "\n",
    "!ls $outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a0d9e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras.bin  cameras.txt  images.bin  images.txt  points3D.bin\tpoints3D.txt\r\n"
     ]
    }
   ],
   "source": [
    "exp5_dir = outputs / 'base'\n",
    "exp5_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!cp -r $poses $exp5_dir\n",
    "!cp -r $camera_model $exp5_dir\n",
    "!touch $exp5_dir/points3D.txt\n",
    "\n",
    "!ls $exp5_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d517fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras.bin  cameras.txt  images.bin  images.txt  points3D.bin\tpoints3D.txt\r\n"
     ]
    }
   ],
   "source": [
    "!colmap model_converter \\\n",
    "    --input_path $exp5_dir \\\n",
    "    --output_path $exp5_dir \\\n",
    "    --output_type BIN\n",
    "\n",
    "!ls $exp5_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dd37429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/12/05 12:39:03 hloc INFO] Extracting local features with configuration:\n",
      "{'model': {'name': 'netvlad'},\n",
      " 'output': 'global-feats-netvlad',\n",
      " 'preprocessing': {'resize_max': 1024}}\n",
      "[2022/12/05 12:39:03 hloc INFO] Found 100 images in root /workspace/datasets/sk3d/dataset/dragon/tis_right/rgb/undistorted/ambient@best.\n",
      "[2022/12/05 12:39:03 hloc INFO] Skipping the extraction.\n",
      "[2022/12/05 12:39:03 hloc INFO] Extracting image pairs from a retrieval database.\n",
      "[2022/12/05 12:39:03 hloc INFO] Found 500 pairs.\n",
      "[2022/12/05 12:39:03 hloc INFO] Extracting semi-dense features with configuration:\n",
      "{'model': {'name': 'superglue',\n",
      "           'sinkhorn_iterations': 50,\n",
      "           'weights': 'outdoor'},\n",
      " 'output': 'matches-superglue'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m global_descriptors \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mmain(retrieval_conf, images, outputs)\n\u001b[1;32m      3\u001b[0m pairs_from_retrieval\u001b[38;5;241m.\u001b[39mmain(global_descriptors, loc_pairs, \u001b[38;5;241m5\u001b[39m, db_model\u001b[38;5;241m=\u001b[39mexp5_dir)\n\u001b[0;32m----> 5\u001b[0m features, loc_matches \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_dense\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatcher_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloc_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_kps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatches\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/pixel-perfect-sfm/Hierarchical-Localization/hloc/match_dense.py:564\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(conf, pairs, image_dir, export_dir, matches, features, features_ref, max_kps, overwrite)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(features_ref))\n\u001b[0;32m--> 564\u001b[0m \u001b[43mmatch_and_assign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfeatures_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_kps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_q, matches\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/pixel-perfect-sfm/Hierarchical-Localization/hloc/match_dense.py:501\u001b[0m, in \u001b[0;36mmatch_and_assign\u001b[0;34m(conf, pairs_path, image_dir, match_path, feature_path_q, feature_paths_refs, max_kps, overwrite)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# extract semi-dense matches\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m \u001b[43mmatch_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexisting_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexisting_refs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssigning matches...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Pre-load existing keypoints\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/pixel-perfect-sfm/Hierarchical-Localization/hloc/match_dense.py:260\u001b[0m, in \u001b[0;36mmatch_dense\u001b[0;34m(conf, pairs, image_dir, match_path, existing_refs)\u001b[0m\n\u001b[1;32m    257\u001b[0m Model \u001b[38;5;241m=\u001b[39m dynamic_load(matchers, conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    258\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 260\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ImagePairDataset(image_dir, \u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, pairs)\n\u001b[1;32m    261\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    262\u001b[0m         dataset, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    264\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming dense matching...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'preprocessing'"
     ]
    }
   ],
   "source": [
    "global_descriptors = extract_features.main(retrieval_conf, images, outputs)\n",
    "\n",
    "pairs_from_retrieval.main(global_descriptors, loc_pairs, 5, db_model=exp5_dir)\n",
    "\n",
    "features, loc_matches = match_dense.main(\n",
    "    matcher_conf, \n",
    "    loc_pairs, \n",
    "    images, \n",
    "    outputs, \n",
    "    features=features, \n",
    "    max_kps=None,\n",
    "    matches=matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c9e8b",
   "metadata": {},
   "source": [
    "В папке **exp5_dir** будут сохранены все результаты данного эксперимента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f8a67",
   "metadata": {},
   "source": [
    "Смотрим при помощи **pycolmap** информацию о полученной на данной момент реконструкции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57090735",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = pycolmap.Reconstruction(exp5_dir)\n",
    "print(reconstruction.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff76a8",
   "metadata": {},
   "source": [
    "Импортируем **hloc**. Она нам поможет создать БД, заполнить БД, сделать геометрическую верификацию (если понадобится)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import hloc\n",
    "except ImportError:\n",
    "    print(\"Could not import hloc.\")\n",
    "    hloc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8269f",
   "metadata": {},
   "source": [
    "1) Создание БД\n",
    "\n",
    "2) Импорт камер, картинок, улучшенных features, matches\n",
    "\n",
    "Ниже клетка выполняется за 1 минуту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b99ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hloc_path = exp5_dir / 'hloc'\n",
    "hloc_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "database_path = hloc_path / 'database.db' \n",
    "\n",
    "print(\"DATABASE PATH -> \", database_path)\n",
    "\n",
    "reference = pycolmap.Reconstruction(exp5_dir)   \n",
    "\n",
    "images_txt_path = exp5_dir / 'images.txt'\n",
    "images_dict = read_images_text(images_txt_path)\n",
    "        \n",
    "# Here I changed code and in database we have data about camera extrinsics    \n",
    "image_ids = hloc.triangulation.create_db_from_model(reference, \n",
    "                                                    database_path, \n",
    "                                                    images_dict\n",
    "                                                    )\n",
    "\n",
    "\n",
    "#Importing features into database -> keypoints table \n",
    "hloc.triangulation.import_features(image_ids, \n",
    "                                   database_path, \n",
    "                                   refined_keypoints)\n",
    "\n",
    "\n",
    "#Importing matches into database -> matches table\n",
    "\n",
    "skip_geometric_verification = False\n",
    "hloc.triangulation.import_matches(image_ids, \n",
    "                                  database_path, \n",
    "                                  sfm_pairs, \n",
    "                                  matches,\n",
    "                                  min_match_score=None, \n",
    "                                  skip_geometric_verification=skip_geometric_verification)\n",
    "#flag skip_geometric_verification == True - add info to two_view_geometry table, same matches are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093526a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "\n",
    "%sql sqlite:////workspace/pixel-perfect-sfm/outputs/dragon/experiment_5/base/hloc/database.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb360b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select count(*) from matches limit 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select sum(rows), count(*) from matches;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_geometric_verification = False\n",
    "\n",
    "estimate_two_view_geometries = True\n",
    "verbose = True\n",
    "\n",
    "if not skip_geometric_verification:\n",
    "        if estimate_two_view_geometries:\n",
    "            hloc.triangulation.estimation_and_geometric_verification(database_path, \n",
    "                                                  sfm_pairs, \n",
    "                                                  verbose)\n",
    "        else:\n",
    "            hloc.triangulation.geometric_verification(\n",
    "                image_ids, \n",
    "                reference, \n",
    "                database_path, \n",
    "                features, \n",
    "                sfm_pairs, \n",
    "                matches)\n",
    "            \n",
    "# output ->  mean/med/min/max valid matches 98.71/99.13/65.79/100.00%.            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select sum(rows), count(*) from two_view_geometries;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select * from matches limit 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT  count(*)\n",
    "FROM    matches\n",
    "JOIN\n",
    "        two_view_geometries\n",
    "ON      (matches.data = two_view_geometries.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c84a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sql\n",
    "\n",
    "# SELECT m.pair_id\n",
    "# FROM matches m\n",
    "# WHERE EXISTS(SELECT g.pair_id \n",
    "# FROM two_view_geometries g\n",
    "# WHERE g.data = m.data and g.pair_id = m.pair_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea14ff",
   "metadata": {},
   "source": [
    "# Ray tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a008f7d",
   "metadata": {},
   "source": [
    "Здесь мы определяем наши 2D keypoints, которые считаем из нашей БД. \n",
    "\n",
    "(Повторяю, тут мы используем уже улучшенные 2D keypoints). \n",
    "\n",
    "2D keypoint в верхнем левом углу верхнего левого пикселя имеет координаты `(0, 0)`, и 2D keypoint в нижнем правом угля нижнего правого пикселя имеет координаты `(width, height)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from pixsfm.util.database import COLMAPDatabase, blob_to_array, pair_id_to_image_ids\n",
    "from pixsfm.base import Map_NameKeypoints\n",
    "\n",
    "\n",
    "def read_matches_from_db_modified(database_path: Path\n",
    "                         ) -> Tuple[List[Tuple[str]], List[np.ndarray]]:\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    id2name = db.image_id_to_name()\n",
    "    desc = {}\n",
    "    for image_id, r, c, data in db.execute(\"SELECT * FROM descriptors\"):\n",
    "        d = blob_to_array(data, np.uint8, (-1, c))\n",
    "        desc[image_id] = d / np.linalg.norm(d, axis=1, keepdims=True)\n",
    "    # only compute scores if descriptors are in database\n",
    "    compute_scores = (len(desc) > 0)\n",
    "    scores = [] if compute_scores else None\n",
    "    pairs = []\n",
    "    matches = []\n",
    "    for pair_id, data in db.execute(\"SELECT pair_id, data FROM two_view_geometries\"):\n",
    "        id1, id2 = pair_id_to_image_ids(pair_id)\n",
    "        name1, name2 = id2name[id1], id2name[id2]\n",
    "        if data is None:\n",
    "            continue\n",
    "        pairs.append((name1, name2))\n",
    "        match = blob_to_array(data, np.uint32, (-1, 2))\n",
    "        matches.append(match)\n",
    "        if compute_scores:\n",
    "            d1, d2 = desc[id1][match[:, 0]], desc[id2][match[:, 1]]\n",
    "            scores.append(np.einsum('nd,nd->n', d1, d2))\n",
    "    db.close()\n",
    "    return pairs, matches, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb55c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixsfm.util.colmap import read_keypoints_from_db, read_matches_from_db, write_keypoints_to_db\n",
    "\n",
    "keypoints = read_keypoints_from_db(database_path)\n",
    "#print(keypoints['0098.png'])\n",
    "\n",
    "pairs, matches, scores = read_matches_from_db_modified(database_path) # the order in matches list is the same as in pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587c6c8",
   "metadata": {},
   "source": [
    "Загружаем данные: скана, модели камеры, позы камер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk3d_root = '/workspace/datasets/sk3d'\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cpu'\n",
    "\n",
    "rec = f'{sk3d_root}/dataset/{scene_name}/stl/reconstruction/cleaned.ply'\n",
    "rec = o3d.io.read_triangle_mesh(rec)\n",
    "\n",
    "occ = f'{sk3d_root}/dataset/{scene_name}/stl/occluded_space.ply'\n",
    "occ = o3d.io.read_triangle_mesh(occ)\n",
    "\n",
    "renderer = MeshRenderer(rec, occ); del occ\n",
    "\n",
    "cam_model = f'{sk3d_root}/dataset/calibration/tis_right/rgb/cameras.txt'\n",
    "cam_model = load_colmap_camera(cam_model).to(device, dtype)\n",
    "\n",
    "poses = f'{sk3d_root}/dataset/{scene_name}/tis_right/rgb/images.txt'\n",
    "world_to_cam = load_poses(poses, dtype).to(device, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ea9f1",
   "metadata": {},
   "source": [
    "Отрисуем 2D keypoints для первого изображения сцены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_i = 0\n",
    "\n",
    "key_pts = torch.tensor(keypoints['0000.png'], dtype=dtype)  # [[x1, y1], [x2, y2], ...]\n",
    "\n",
    "# Visualize keypoints\n",
    "img = f'{sk3d_root}/dataset/{scene_name}/tis_right/rgb/undistorted/ambient@best/{view_i:04}.png'\n",
    "img = imgio.read.tis_right.rgb(img)\n",
    "\n",
    "plt.figure(figsize=[15]*2)\n",
    "plt.imshow(img)\n",
    "\n",
    "print(key_pts)\n",
    "\n",
    "plt.scatter(key_pts[:, 0], key_pts[:, 1], marker='x', c='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0e1a6",
   "metadata": {},
   "source": [
    "Трассировка лучей. Находим 2D keypoints с конечной hit depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_3d_points = {}\n",
    "\n",
    "for image_id, image in reconstruction.images.items():\n",
    "    \n",
    "    key_pts = torch.tensor(keypoints[image.name], dtype=dtype)\n",
    "\n",
    "    ray_dirs_in_cam_space = cam_model.unproject(key_pts.T.to(device, dtype))  # shape is 3, pts_n\n",
    "    pts_n = ray_dirs_in_cam_space.shape[1]\n",
    "\n",
    "    cam_to_world = world_to_cam[view_i].inverse()  # 4, 4\n",
    "    cam_center_in_world_space = cam_to_world[:3, 3]  # 3\n",
    "    ray_origins_in_world_space = cam_center_in_world_space.unsqueeze(0).expand(pts_n, -1)  # pts_n, 3\n",
    "    cam_to_world_rot = cam_to_world[:3, :3]\n",
    "    ray_dirs_in_world_space = ray_dirs_in_cam_space.T @ cam_to_world_rot.T  # pts_n, 3\n",
    "\n",
    "    casted_rays = torch.cat([ray_origins_in_world_space, ray_dirs_in_world_space], 1)  # pts_n, 6\n",
    "    renderer.occ_threshold = 1e-3\n",
    "    \n",
    "    hit_depth = renderer.render_rays(casted_rays, cull_back_faces=True)['ray_hit_depth']\n",
    "    \n",
    "    did_hit = hit_depth.isfinite()\n",
    "    idxs_with_inf = (did_hit==False).nonzero().squeeze()\n",
    "    \n",
    "    pts_3d = (ray_dirs_in_world_space * hit_depth.unsqueeze(1) + ray_origins_in_world_space) \n",
    "    inf_array = np.asarray([np.inf, np.inf, np.inf])\n",
    "    \n",
    "    pts_3d[idxs_with_inf] = torch.from_numpy(inf_array).to(pts_3d) \n",
    "    found_3d_points[image.name] = list(pts_3d.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f077fa",
   "metadata": {},
   "source": [
    "Save and check.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41efd70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.io.write_triangle_mesh(str(outputs / 'scene.ply'), rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (name1, name2) in tqdm(enumerate(pairs)):\n",
    "#     indices = [] \n",
    "#     collect_matches = []\n",
    "#     for k, (idx1, idx2) in enumerate(list(matches[idx])):\n",
    "#         if idx1 not in idx_to_remove_dict[name1] and idx2 not in idx_to_remove_dict[name2]:\n",
    "#             collect_matches.append(np.asarray([idx1, idx2]))\n",
    "#     matches[idx] = np.asarray(collect_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tracks = outputs / 'result' \n",
    "result_tracks.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9e9de",
   "metadata": {},
   "source": [
    "# Tentative matching tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474b098",
   "metadata": {},
   "source": [
    "Создаем объект граф при помощи известных нам **pairs**, **matches**, **scores**. scores идут как None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixsfm.keypoint_adjustment import build_matching_graph\n",
    "from pixsfm import base, features, logger\n",
    "\n",
    "graph = base.Graph()\n",
    "scores = [None for _ in matches]\n",
    "\n",
    "for (name1, name2), m, s in zip(pairs, matches, scores):\n",
    "    graph.register_matches(name1, name2, m, s)   \n",
    "\n",
    "print(\"Graph object \", graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0fadd",
   "metadata": {},
   "source": [
    "Здесь мы создаем словарь, ключом которого является **track_label**, а значением список индексов его **nodes**.\n",
    "\n",
    "В объекте граф есть **graph.nodes**. Это список всех nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80064c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [0, 1, 6464]:\n",
    "#     print(graph.nodes[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adffb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label graph\n",
    "track_labels = base.compute_track_labels(graph)\n",
    "\n",
    "track_labels_with_idxs = defaultdict(list)\n",
    "\n",
    "for i, track_label in enumerate(track_labels):\n",
    "    track_labels_with_idxs[track_label].append(i)\n",
    "\n",
    "pp.pprint(track_labels_with_idxs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## working with just one track (example)\n",
    "\n",
    "def find_3d_for_2d(image_id, \n",
    "                   feature_idx, \n",
    "                   points_3d):\n",
    "\n",
    "    isBadPoint = False\n",
    "    image_name = graph.image_id_to_name[image_id]\n",
    "    \n",
    "    if image_name not in points_3d:\n",
    "        return None, True\n",
    "        \n",
    "    def is_inf_point(pts_3d):\n",
    "        return np.allclose(np.isfinite(pts_3d), [False, False, False])\n",
    "    \n",
    "    pnt_3d = points_3d[image_name][feature_idx]\n",
    "    if is_inf_point(pnt_3d): isBadPoint = True    \n",
    "        \n",
    "    return pnt_3d, isBadPoint\n",
    "\n",
    "\n",
    "def collect_3d_points(graph, \n",
    "                     track_idxs,\n",
    "                     found_3d_points,):\n",
    "    \n",
    "    \"\"\"\n",
    "        Collecting 3d points from tracks with same labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    pts_in_tracks = defaultdict(list) # 3d-points\n",
    "    \n",
    "    for track_label in tqdm(track_idxs):\n",
    "        idxs = track_idxs[track_label]\n",
    "        \n",
    "#         nodes = set()\n",
    "\n",
    "        for i in idxs:\n",
    "            feature_node = graph.nodes[i]      \n",
    "            \n",
    "#             node_idx = feature_node.node_idx\n",
    "#             nodes.add(node_idx)\n",
    "            \n",
    "            pnt_3d, isBadPoint = find_3d_for_2d(feature_node.image_id, \n",
    "                                                feature_node.feature_idx, \n",
    "                                                found_3d_points)\n",
    "           \n",
    "            if pnt_3d is None and isBadPoint:\n",
    "                continue\n",
    "            \n",
    "            elif not isBadPoint:  \n",
    "                pts_in_tracks[track_label].append(pnt_3d)\n",
    "                #print(track_label, feature_node, pnt_3d)\n",
    "            else:\n",
    "                pass\n",
    "   \n",
    "    return pts_in_tracks           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_in_tracks = collect_3d_points(graph, \n",
    "                                  track_labels_with_idxs,\n",
    "                                  found_3d_points,  \n",
    "                                  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_in_tracks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1109, 1041]:\n",
    "    print(track_labels_with_idxs[i], len(track_labels_with_idxs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1109, 1041]:\n",
    "    print(i, pts_in_tracks[i], len(pts_in_tracks[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d6c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "fig, axs = plt.subplots(20,5, figsize=(90, 350), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .1, wspace=.001)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "first100pairs = {k: pts_in_tracks[k] for k in list(pts_in_tracks)[:100]}\n",
    "\n",
    "for i, (track_label, pts) in enumerate(first100pairs.items()):\n",
    "\n",
    "    dist_matrix = distance.cdist(pts, pts, 'euclidean')\n",
    "    \n",
    "    upper_triangle_values = dist_matrix[np.triu_indices(len(pts), k = 1)]\n",
    "    conf_upper_bound = get_conf_upper_bound(upper_triangle_values)\n",
    "    \n",
    "    bins = 10\n",
    "    if len(upper_triangle_values) > 10: bins = 20\n",
    "\n",
    "    axs[i].hist(upper_triangle_values, bins=bins)\n",
    "    axs[i].axvline(conf_upper_bound, color='r', linestyle='--')\n",
    "    axs[i].set_title(str(track_label))\n",
    "    \n",
    "fig.savefig(str(result_tracks / 'distribution_of_distances.pdf'))    \n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8692ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "dists = []\n",
    "\n",
    "for i, (track_label, pts) in enumerate(pts_in_tracks.items()):\n",
    "\n",
    "    dist_matrix = distance.cdist(pts, pts, 'euclidean')\n",
    "    upper_triangle_values = dist_matrix[np.triu_indices(len(pts), k = 1)]\n",
    "    \n",
    "    if len(upper_triangle_values) != 0:\n",
    "        dists.append(upper_triangle_values)\n",
    "        \n",
    "# conf_upper_bound = get_conf_upper_bound(upper_triangle_values)      \n",
    "dists = [item for sublist in dists for item in sublist]\n",
    "plt.hist(dists, bins=100)\n",
    "plt.title(\"Distrbution of distances between points in all tracks\")\n",
    "\n",
    "fig.savefig(str(result_tracks / 'distribution_of_distances_in_all_tracks.pdf'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm $result_tracks/pts_in_tracks_without_matches.json\n",
    "\n",
    "_pts_in_tracks_without_matches = {}    \n",
    "\n",
    "for k, val in pts_in_tracks.items():\n",
    "    new_list = [el.tolist() for el in val]\n",
    "    _pts_in_tracks_without_matches[k] = new_list \n",
    "\n",
    "with open( str(result_tracks / 'pts_in_tracks_without_matches.json'), 'w') as fp:\n",
    "    json.dump(_pts_in_tracks_without_matches, fp)  \n",
    "\n",
    "del _pts_in_tracks_without_matches    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3c55a",
   "metadata": {},
   "source": [
    "### Checking track as a pointcloud in meshlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae930e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointclouds = outputs / 'pointclouds'\n",
    "pointclouds.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!rm -r $pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointclouds_tracks = outputs / 'pointclouds_tracks'\n",
    "\n",
    "!rm -r $pointclouds_tracks\n",
    "pointclouds_tracks.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (track_label, pts) in enumerate(pts_in_tracks.items()):\n",
    "    \n",
    "    points = [p.tolist() for p in pts]\n",
    "\n",
    "    pc = o3d.geometry.PointCloud()\n",
    "    pc.points.extend(points)\n",
    "\n",
    "    o3d.io.write_point_cloud(str(pointclouds_tracks / f'{track_label}_track.ply'), pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fccf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff625a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, (track_label, pts) in enumerate(pts_in_tracks.items()):\n",
    "    \n",
    "    _pts_in_tracks = copy.copy(pts_in_tracks)\n",
    "    \n",
    "    print(f\"\\nTRACK_LABEL_{track_label}\")\n",
    "    score_labels = base.compute_score_labels(graph, track_labels)\n",
    "\n",
    "    scores_for_test = {}\n",
    "    idxs = track_labels_with_idxs[track_label] \n",
    "    print(\"idxs\",idxs,  \"\\n\")\n",
    "\n",
    "    for i in idxs: scores_for_test[i] = score_labels[i]\n",
    "\n",
    "    print(scores_for_test)\n",
    "    break\n",
    "        \n",
    "    sorted_by_score = dict(sorted(scores_for_test.items(), key=lambda item: item[1], reverse = True))\n",
    "    print(sorted_by_score)\n",
    "    \n",
    "    for k, score in sorted_by_score.items():\n",
    "        print(k, len(_pts_in_tracks[k]))\n",
    "        \n",
    "        if len(_pts_in_tracks[k]) != 0:\n",
    "\n",
    "            points = [p.tolist() for p in _pts_in_tracks[k]]\n",
    "            \n",
    "            pc = o3d.geometry.PointCloud()\n",
    "            pc.points.extend(points)\n",
    "            \n",
    "            track_folder = pointclouds / f'{track_label}'\n",
    "            track_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            o3d.io.write_point_cloud(str(track_folder / f'{track_label}_idx_{k}_score_{score}.ply'), pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (track_label, pts) in enumerate(pts_in_tracks.items()):\n",
    "    \n",
    "#     _pts_in_tracks = copy.copy(pts_in_tracks)\n",
    "    \n",
    "#     print(f\"\\nTRACK_LABEL_{track_label}\")\n",
    "#     score_labels = base.compute_score_labels(graph, track_labels)\n",
    "\n",
    "#     scores_for_test = {}\n",
    "#     idxs = track_labels_with_idxs[track_label] \n",
    "#     print(\"idxs\",idxs,  \"\\n\")\n",
    "\n",
    "#     for i in idxs: scores_for_test[i] = score_labels[i]\n",
    "\n",
    "#     print(scores_for_test)\n",
    "#     break\n",
    "        \n",
    "#     sorted_by_score = dict(sorted(scores_for_test.items(), key=lambda item: item[1], reverse = True))\n",
    "#     print(sorted_by_score)\n",
    "    \n",
    "#     for k, score in sorted_by_score.items():\n",
    "#         print(k, len(_pts_in_tracks[k]))\n",
    "        \n",
    "#         if len(_pts_in_tracks[k]) != 0:\n",
    "\n",
    "#             points = [p.tolist() for p in _pts_in_tracks[k]]\n",
    "            \n",
    "#             pc = o3d.geometry.PointCloud()\n",
    "#             pc.points.extend(points)\n",
    "            \n",
    "#             track_folder = pointclouds / f'{k}'\n",
    "#             track_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "#             o3d.io.write_point_cloud(str(track_folder / f'{track_label}_idx_{k}_score_{score}.ply'), pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658adf6",
   "metadata": {},
   "source": [
    "### Plotting the distribution of points distances in each track and all tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27da9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot number of points for each track\n",
    "\n",
    "fig, ax1 = plt.subplots(1)\n",
    "\n",
    "points_num_with = [len(value) for key, value in pts_in_tracks.items()]\n",
    "# points_num_without = [len(value) for key, value in pts_in_tracks_without_matches.items()]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (22,8)\n",
    "ax1.hist(points_num_with, bins=100, alpha=0.7, label='with matches')\n",
    "# ax2.hist(points_num_without, bins=100, alpha=0.3, label='without matches')\n",
    "\n",
    "ax1.set_xlabel(\"# of points in track\", fontsize = 14)\n",
    "# ax2.set_xlabel(\"# of points in track\", fontsize = 14)\n",
    "\n",
    "ax1.legend()\n",
    "# ax2.legend()\n",
    "\n",
    "ax1.set_ylabel(\"Frequency\", fontsize = 14)\n",
    "# ax2.set_ylabel(\"Frequency\", fontsize = 14)\n",
    "\n",
    "fig.suptitle('Histogram of number of points in the tracks')\n",
    "\n",
    "fig.savefig(str( result_tracks / 'number_of_points_in_the_tracks.pdf') )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232bf12d",
   "metadata": {},
   "source": [
    "### Looking at tracks with highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node within each track with highest score\n",
    "\n",
    "root_labels = base.compute_root_labels(graph, track_labels, score_labels)\n",
    "indices = [i for i, x in enumerate(root_labels) if x]\n",
    "\n",
    "highest_score_tracks = {}\n",
    "\n",
    "for i in indices:\n",
    "    track_label = track_labels[i]\n",
    "    highest_score_tracks[track_label] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ad5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_in_highest_tracks = collect_3d_points(graph, \n",
    "                                  highest_score_tracks,\n",
    "                                  found_3d_points,  \n",
    "                                  addMatches=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38470068",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_in_highest_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879308ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(list(pts_in_highest_tracks.values()), axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_in_highest_tracks[466]\n",
    "track_labels_with_idxs[466]   \n",
    "\n",
    "\n",
    "def collect_feature_node_ids(track_labels_with_idxs):\n",
    "    result_dict = defaultdict(list)\n",
    "    for track_label, idxs in track_labels_with_idxs.items():\n",
    "        for i in idxs:\n",
    "            featurenode = graph.nodes[i]\n",
    "            feature_num_matches = featurenode.num_matches\n",
    "            \n",
    "            feature_node_idx = featurenode.node_idx\n",
    "            print(featurenode)\n",
    "            break\n",
    "            result_dict[track_label].append(featurenode_idx)\n",
    "        break    \n",
    "\n",
    "collect_feature_node_ids(track_labels_with_idxs)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnts = [ el[0] for el in list(pts_in_highest_tracks.values())]\n",
    "\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points.extend(pnts)\n",
    "\n",
    "print(len(pnts))\n",
    "\n",
    "draw_geometries([pc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e31b8",
   "metadata": {},
   "source": [
    "## Method 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update points in track by mean value of given 3d points\n",
    "\n",
    "upd_pts_method_1 = {}\n",
    "\n",
    "for track_label, pts in pts_in_tracks.items():\n",
    "    if len(pts) == 1:\n",
    "        upd_pts_method_1[track_label] = pts\n",
    "    if len(pts) >= 2:\n",
    "        mean_pnt = np.mean(pts, axis = 0)\n",
    "        upd_pts_method_1[track_label] = mean_pnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ecb85",
   "metadata": {},
   "source": [
    "## Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "method1 = result_tracks / 'method1'\n",
    "method1.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421a5db",
   "metadata": {},
   "source": [
    "Ниже клетка выполняется за 45 минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "#  From distance fucntion distance matrix is returned. \n",
    "# For each i and j, the metric dist(u=XA[i], v=XB[j]) is computed and stored in the i,j entry.\n",
    "\n",
    "from welzl import welzl\n",
    "\n",
    "# compute the NxN inter-distances in O(N^2)\n",
    "# sum each row of this matrix (= the distance of one point to the others) in O(N^2)\n",
    "# sort the obtained \"crowd\" distance in O(N*log N)\n",
    "# (the point with smallest distance is an approximation of the geometric median)\n",
    "# remove the 5% largest in O(1)\n",
    "# here we just consider largest crowd-distance as outliers,\n",
    "# instead of taking the largest distance from the median.\n",
    "# compute radius of obtained sphere in O(N)\n",
    "# Of course, it also suffers from sub-optimality but should perform a bit better in case of far outlier.\n",
    "# Overall cost is O(N^2).\n",
    "\n",
    "upd_pts_method_1 = {}\n",
    "radiuses_method_1 = []\n",
    "\n",
    "\n",
    "for track_label, pts in tqdm(pts_in_tracks.items()):\n",
    "    \n",
    "    percentile_95 = round(0.95 * len(pts))\n",
    "    dist_matrix = distance.cdist(pts, pts, 'euclidean')\n",
    "\n",
    "    row_sum = np.sum(dist_matrix, axis=1)\n",
    "    sorted_row_sum = np.sort(row_sum)    \n",
    "    \n",
    "    # here we just consider largest crowd-distance as outliers,\n",
    "    # instead of taking the largest distance from the median.\n",
    "    outliers_distances = sorted_row_sum[percentile_95:]\n",
    "    \n",
    "    idx_to_remove = []\n",
    "    for dst in outliers_distances:\n",
    "        result = np.where(row_sum == dst)[0][0]\n",
    "        idx_to_remove.append(result)\n",
    "    \n",
    "    upd_pts =  []\n",
    "    for i, p in enumerate(pts):\n",
    "        if i not in idx_to_remove:\n",
    "            upd_pts.append(p)\n",
    "            \n",
    "    nsphere = welzl(upd_pts)\n",
    "    \n",
    "    r = np.sqrt(nsphere.sqradius)\n",
    "    cent = nsphere.center\n",
    "    \n",
    "    print(\"Track labes is \", track_label)\n",
    "    print(\"    Center is at: \", cent)\n",
    "    print(\"    Radius is: \", r, \"\\n\")\n",
    "    \n",
    "    upd_pts_method_1[track_label] = cent\n",
    "    radiuses_method_1.append(r)   \n",
    "    \n",
    "    del upd_pts\n",
    "    \n",
    "    \n",
    "upd_pts_method_1 = {k: upd_pts_method_1[k].tolist() for k in list(upd_pts_method_1)}    \n",
    "\n",
    "with open( str(method1 / 'method1_with_matches.json'), 'w') as fp:\n",
    "    data = []\n",
    "    data.append({\"radiuses\": radiuses_method_1})\n",
    "    data.append({\"3d_points\": upd_pts_method_1})\n",
    "    json.dump(data, fp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_distribution_of_radiuses(radiuses,\n",
    "                                   method_num,\n",
    "                                  path_to_method):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "    x = radiuses\n",
    "    y = range(len(radiuses))\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "\n",
    "    fig.set_figwidth(8)\n",
    "    fig.set_figheight(15)\n",
    "\n",
    "    ax1.set_title(f\"Method {method_num}\", fontsize=20)\n",
    "    ax1.scatter(x, y)\n",
    "    ax1.legend()\n",
    "\n",
    "    ########################\n",
    "\n",
    "    data = list(filter(lambda x: x != 0, radiuses)) # without 0 distance here!\n",
    "\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    print(\"Mean \", mean)\n",
    "    print(\"Median \", median)\n",
    "\n",
    "    ax2.axvline(mean, color='r', linestyle='--')\n",
    "    ax2.axvline(median, color='g', linestyle='-')\n",
    "\n",
    "    ax2.hist(data, bins=50, density=True)\n",
    "\n",
    "    ax2.set_xlabel(\"Radiuses of smallest bounding spheres\")\n",
    "    ax2.set_ylabel(\"Quantity\")\n",
    "    ax2.set_title(\"Distribution of found radiuses\", fontsize=12)\n",
    "    ax2.plot([], [], ' ', label=f\"Mean: {mean}\")\n",
    "    ax2.plot([], [], ' ', label=f\"Median: {median}\")\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "    #######################\n",
    "\n",
    "    data_0 = radiuses\n",
    "\n",
    "    mean = np.mean(data_0)\n",
    "    median = np.median(data_0)\n",
    "    print(\"Mean \", mean)\n",
    "    print(\"Median \", median)\n",
    "\n",
    "    ax3.axvline(mean, color='r', linestyle='--')\n",
    "    ax3.axvline(median, color='g', linestyle='-')\n",
    "\n",
    "    ax3.hist(data_0, bins=50, density=True)\n",
    "\n",
    "    ax3.set_xlabel(\"Radiuses of smallest bounding spheres (with info about 0)\")\n",
    "    ax3.set_ylabel(\"Quantity\")\n",
    "    ax3.plot([], [], ' ', label=f\"Mean: {mean}\")\n",
    "    ax3.plot([], [], ' ', label=f\"Median: {median}\")\n",
    "\n",
    "    ax3.legend()\n",
    "\n",
    "    fig.savefig(str( path_to_method / 'distribution_of_found_radiuses.pdf') )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distribution_of_radiuses(radiuses_method_1, 1, method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56616ee3",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85743d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method2 = result_tracks / 'method2'\n",
    "method2.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e35fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, nct\n",
    "\n",
    "track_num = 1141\n",
    "\n",
    "# a demo sample\n",
    "x = [0.02130260546341465, 0.021878736423016517, 0.024428375041446026, 0.03167656248812453, \n",
    "     0.021607927798428992, 0.02218236952747051, 0.02938439390419522, 0.021895384443549402, \n",
    "     0.0208653394740238, 0.022615560017453837, 0.026069874576700663, 0.020490277066658413, \n",
    "     0.02276381086366452, 0.02741631464168221, 0.023065996816853965, 0.027920032580417257, \n",
    "     0.029783724542362115, 0.018672376360816232, 0.02411919130687289, 0.024032041697362883, \n",
    "     0.027668555080877243, 0.029038746866940128, 0.028905096038895173, 0.028455422389231457, \n",
    "     0.02777097119444502, 0.02645934696996008, 0.020406710342586114, 0.018363611315954646, \n",
    "     0.029764300669835594, 0.028150384772774876, 0.028483541898183005, 0.026539612194819227, \n",
    "     0.024770634678803787, 0.0256449404014951, 0.02701950540179652, 0.02800481701574395, \n",
    "     0.026255726719030262, 0.026822962223460232, 0.02795852919052212, 0.02867081143636126, \n",
    "     0.02802730758510055, 0.026939408443483283, 0.02756843844933439, 0.026711061791156974, \n",
    "     0.028354813127200527, 0.02717413194036283, 0.027048786517346646, 0.02093044948575734, \n",
    "     0.02828042330464175, 0.028437914098171705, 0.029003609002851094, 0.02791780539527151, \n",
    "     0.028315803118693243, 0.02760123113501837, 0.02773386028268046, 0.02857483708473208,\n",
    "     0.029392926114190886, 0.027680620974533866, 0.020846311414102706, 0.021942669842570775, \n",
    "     0.02800255456735636, 0.028071564643575363, 0.019107524162219267, 0.02829224661377798, \n",
    "     0.028648979770239322, 0.028379039104801607, 0.02895200332650313, 0.027743450215637434, \n",
    "     0.02844947225303563, 0.028480928942857968, 0.027493861704528902, 0.03167656264724666]\n",
    "\n",
    "n = len(x)\n",
    "\n",
    "print(\"Sample size \", n)\n",
    "\n",
    "conf_upper_bound = get_conf_upper_bound(x)\n",
    "print(\"One-sided tolerance upper bound\", conf_upper_bound)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 5))\n",
    "plt.axvline(conf_upper_bound, color='r', linestyle='--')\n",
    "plt.hist(x, bins=len(x), density=True)\n",
    "\n",
    "\n",
    "plt.title(f\"Example for track number {track_num}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d26b3",
   "metadata": {},
   "source": [
    "Ниже клетка работает 1 час."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6043dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update points via smallest bounding sphere (experiment 2) without percentile???\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from welzl import welzl, dist\n",
    "\n",
    "# I would iterate the following two steps until convergence:\n",
    "\n",
    "# 1) Given a group of points, find the smallest sphere enclosing 100% of the points and work out its centre.\n",
    "# 2) Given a centre, find the group of points containing 95% of the original number which is closest to the centre.\n",
    "# Each step reduces (or at least does not increase) the radius of the sphere involved, so you can declare convergence when the radius stops decreasing.\n",
    "# In fact, I would iterate from multiple random starts, each start produced by finding the smallest sphere that contains all of a small subset of points.\n",
    "# I note that if you have 10 outliers, and you divide your set of points into 11 parts, at least one of those parts will not have any outliers.\n",
    "# (This is very loosely based on https://en.wikipedia.org/wiki/Random_sample_consensus)\n",
    "\n",
    "upd_pts_method_2 = {}\n",
    "radiuses_method_2 = []\n",
    "\n",
    "for track_label, pts in tqdm(pts_in_tracks.items()):\n",
    "    \n",
    "    nsphere = welzl(pts)\n",
    "#     print(\"Track label is \", track_label)\n",
    "#     print(\"    Center is at: \", nsphere.center)\n",
    "#     print(\"    Radius is: \", np.sqrt(nsphere.sqradius),)\n",
    "    \n",
    "    center = nsphere.center\n",
    "    dists_to_center = [dist(p, center) for p in pts]\n",
    "    sort_dists = np.sort(dists_to_center)\n",
    "\n",
    "    upper_bound = get_conf_upper_bound(dists_to_center)\n",
    "    out_indx = [i for i,v in enumerate(dists_to_center) if v > upper_bound]\n",
    "    \n",
    "    upd_pts =  []\n",
    "    \n",
    "    if len(out_indx) != 0:\n",
    "        for i, p in enumerate(pts):\n",
    "            if i not in out_indx:\n",
    "                upd_pts.append(p)\n",
    "    else:\n",
    "        upd_pts = pts\n",
    "             \n",
    "            \n",
    "    nsphere = welzl(upd_pts)\n",
    "    #print(\"For points: \", upd_pts)\n",
    "    print(\"Track labes is \", track_label)\n",
    "    print(\"    Center is at: \", nsphere.center)\n",
    "    print(\"    Radius is: \", np.sqrt(nsphere.sqradius), \"\\n\")\n",
    "    \n",
    "    upd_pts_method_2[track_label] = nsphere.center\n",
    "    radiuses_method_2.append(np.sqrt(nsphere.sqradius))   \n",
    "    \n",
    "    \n",
    "upd_pts_method_2 = {k: upd_pts_method_2[k].tolist() for k in list(upd_pts_method_2)}    \n",
    "\n",
    "with open( str(method2 / 'method2_with_matches.json'), 'w') as fp:\n",
    "    data = []\n",
    "    data.append({\"radiuses\": radiuses_method_2})\n",
    "    data.append({\"3d_points\": upd_pts_method_2})\n",
    "    json.dump(data, fp)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15273cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distribution_of_radiuses(radiuses_method_2,\n",
    "                                   2,\n",
    "                                  method2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3864ba",
   "metadata": {},
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef88b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "method3 = result_tracks / 'method3'\n",
    "method3.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14230da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stat with sum of pair-wise distances\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from welzl import welzl\n",
    "\n",
    "#  From distance fucntion distance matrix is returned. \n",
    "# For each i and j, the metric dist(u=XA[i], v=XB[j]) is computed and stored in the i,j entry.\n",
    "\n",
    "# compute the NxN inter-distances in O(N^2)\n",
    "# sum each row of this matrix (= the distance of one point to the others) in O(N^2)\n",
    "# sort the obtained \"crowd\" distance in O(N*log N)\n",
    "# (the point with smallest distance is an approximation of the geometric median)\n",
    "# remove the 5% largest in O(1) with statistics here\n",
    "# compute radius of obtained sphere in O(N)\n",
    "# Of course, it also suffers from sub-optimality but should perform a bit better in case of far outlier.\n",
    "# Overall cost is O(N^2).\n",
    "\n",
    "upd_pts_method_3 = {}\n",
    "radiuses_method_3 = []\n",
    "init_radiuses = []\n",
    "\n",
    "for track_label, pts in tqdm(pts_in_tracks.items()):\n",
    "    \n",
    "    nsphere = welzl(upd_pts)\n",
    "    init_radiuses.append(nsphere.center)\n",
    "    \n",
    "    dist_matrix = distance.cdist(pts, pts, 'euclidean')\n",
    "\n",
    "    row_sum = np.sum(dist_matrix, axis=1)\n",
    "    #print(row_sum)\n",
    "\n",
    "    sorted_row_sum = np.sort(row_sum)    \n",
    "#     print(\"Distances: \", sorted_row_sum)\n",
    "    \n",
    "    upper_bound = get_conf_upper_bound(row_sum)\n",
    "    out_indx = [i for i,v in enumerate(row_sum) if v > upper_bound]\n",
    "    \n",
    "    upd_pts = []\n",
    "    if len(out_indx) != 0:\n",
    "        for i, p in enumerate(pts):\n",
    "            if i not in out_indx:\n",
    "                upd_pts.append(p)\n",
    "    else:\n",
    "        upd_pts = pts\n",
    "    \n",
    "    nsphere = welzl(upd_pts)\n",
    "    \n",
    "    print(\"Track labes is \", track_label)\n",
    "    print(\"    Center is at: \", nsphere.center)\n",
    "    print(\"    Radius is: \", np.sqrt(nsphere.sqradius), \"\\n\")\n",
    "    \n",
    "    upd_pts_method_3[track_label] = nsphere.center\n",
    "    radiuses_method_3.append(np.sqrt(nsphere.sqradius))    \n",
    "    \n",
    "upd_pts_method_3 = {k: upd_pts_method_3[k].tolist() for k in list(upd_pts_method_3)}    \n",
    "\n",
    "with open( str(method3 / 'method3_with_matches.json'), 'w') as fp:\n",
    "    data = []\n",
    "    data.append({\"radiuses\": radiuses_method_3})\n",
    "    data.append({\"3d_points\": upd_pts_method_3})\n",
    "    json.dump(data, fp)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distribution_of_radiuses(radiuses_method_3, 3, method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/how-to-plot-normal-distribution-over-histogram-in-python/\n",
    "\n",
    "data1 = radiuses_method_1\n",
    "data2 = radiuses_method_2\n",
    "data3 = radiuses_method_3\n",
    "\n",
    "plt.hist(data1, bins=20, alpha=0.7, label='method1')\n",
    "plt.hist(data2, bins=20,  alpha=0.3, label='method2')\n",
    "plt.hist(data3, bins=20,  alpha=0.8, label='method3')\n",
    "\n",
    "plt.xlabel(\"Radiuses of smallest bounding spheres\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.legend()\n",
    "plt.title(f\"Distribution of found radiuses\", fontsize=18)\n",
    "\n",
    "plt.savefig(str(result_tracks / f'distribution_of_radiuses_in_3_methods.pdf'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5b5d5",
   "metadata": {},
   "source": [
    "## Method  4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "method4 = result_tracks / 'method4'\n",
    "method4.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9b646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "data = np.asarray([el.tolist() for el in pts_in_tracks[1041]])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:,0], data[:,1], data[:,2], s=300)\n",
    "ax.view_init(azim=200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# eps - The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "# This is not a maximum bound on the distances of points within a cluster. \n",
    "# This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
    "\n",
    "model = DBSCAN(eps=0.01, min_samples=2)\n",
    "model.fit_predict(data)\n",
    "pred = model.fit_predict(data)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:,0], data[:,1], data[:,2], c=model.labels_, s=300)\n",
    "ax.view_init(azim=200)\n",
    "plt.show()\n",
    "\n",
    "print(\"number of cluster found: {}\".format(len(set(model.labels_))))\n",
    "print('cluster for each point: ', model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e566e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fcbc248",
   "metadata": {},
   "source": [
    "## Method 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "method5 = result_tracks / 'method5'\n",
    "method5.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_pts_method_5 = {}\n",
    "radiuses_method_5 = []\n",
    "init_radiuses = []\n",
    "\n",
    "for track_label, pts in tqdm(pts_in_tracks.items()):\n",
    "    \n",
    "    print(\"Track label is \", track_label, \" len(points): \", len(pts))\n",
    "    \n",
    "    if len(pts) in list(range(3)):\n",
    "        \n",
    "        nsphere = welzl(upd_pts)\n",
    "        sphere_center = nsphere.center\n",
    "        sphere_radius = np.sqrt(nsphere.sqradius)\n",
    "        \n",
    "        print(\"    Center is at: \", sphere_center)\n",
    "        print(\"    Radius is: \", sphere_radius, \"\\n\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        _pts = copy.deepcopy(pts)\n",
    "        \n",
    "        nsphere = welzl(_pts)\n",
    "        prev_radius = np.sqrt(nsphere.sqradius)\n",
    "        new_radius = 0\n",
    "        new_center = 0\n",
    "        \n",
    "        MAX_ITERS = 5\n",
    "        \n",
    "        while MAX_ITERS > 0 and abs(prev_radius - new_radius) >= 0.001:\n",
    "\n",
    "            radiuses = []\n",
    "            new_set_points = []\n",
    "\n",
    "            for i, point in enumerate(_pts):\n",
    "                new_set_points = [_pts[k] for k in range(len(_pts)) if k != i]\n",
    "                new_nsphere = welzl(new_set_points)\n",
    "                radiuses.append(new_nsphere.sqradius)\n",
    "\n",
    "            index_max = np.argmax(radiuses)\n",
    "            _pts.pop(index_max)  \n",
    "            \n",
    "            \n",
    "            nsphere = welzl(_pts)\n",
    "            new_center = nsphere.center\n",
    "            new_radius = np.sqrt(nsphere.sqradius)\n",
    "            \n",
    "            if new_radius < prev_radius:\n",
    "                prev_radius = new_radius\n",
    "            \n",
    "            \n",
    "            MAX_ITERS -= 1\n",
    "        \n",
    "        nsphere = welzl(_pts)\n",
    "        sphere_center = nsphere.center\n",
    "        sphere_radius = np.sqrt(nsphere.sqradius)\n",
    "\n",
    "\n",
    "    print(\"    Center is at: \", nsphere.center)\n",
    "    print(\"    Radius is: \", np.sqrt(nsphere.sqradius), \"\\n\")\n",
    "    \n",
    "    upd_pts_method_5[track_label] = sphere_center \n",
    "    radiuses_method_5.append(sphere_radius)      \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "# upd_pts_method_5 = {k: upd_pts_method_5[k].tolist() for k in list(upd_pts_method_5)}    \n",
    "\n",
    "# with open( str(method5 / 'method5.json'), 'w') as fp:\n",
    "#     data = []\n",
    "#     data.append({\"radiuses\": radiuses_method_5})\n",
    "#     data.append({\"3d_points\": upd_pts_method_5})\n",
    "#     json.dump(data, fp)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1745c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distribution_of_radiuses(radiuses_method_5, 5, method5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/how-to-plot-normal-distribution-over-histogram-in-python/\n",
    "\n",
    "data1 = radiuses_method_1\n",
    "data2 = radiuses_method_2\n",
    "data3 = radiuses_method_3\n",
    "data5 = radiuses_method_5\n",
    "\n",
    "plt.hist(data1, bins=50, alpha=0.7, label='method1')\n",
    "plt.hist(data2, bins=50,  alpha=0.3, label='method2')\n",
    "plt.hist(data3, bins=50,  alpha=0.9, label='method3')\n",
    "plt.hist(data5, bins=50,  alpha=0.5, label='method5')\n",
    "\n",
    "plt.xlabel(\"Radiuses of smallest bounding spheres\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.legend()\n",
    "plt.title(f\"Distribution of found radiuses\", fontsize=18)\n",
    "\n",
    "plt.savefig(str(result_tracks / f'distribution_of_radiuses_in_4_methods.pdf'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382f932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a16b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc30bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48dfe80a",
   "metadata": {},
   "source": [
    "I compute track labels to see how many graph nodes, grap edges, tracks I have. Each track has its own index, here we have 1310 tracks,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293b86f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Label graph\n",
    "track_labels = base.compute_track_labels(graph)\n",
    "\n",
    "track_count = Counter(track_labels)\n",
    "print(track_count, len(track_count))\n",
    "\n",
    "print(\"Track labels length: \", len(track_labels), \"\\nMax track_label: \", max(track_labels))\n",
    "\n",
    "max_per_problem = max(track_count.values())\n",
    "print(\"max_per_problem \", max_per_problem, sorted(track_count.keys()))\n",
    "print(sum(track_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score to connected features within same track\n",
    "score_labels = base.compute_score_labels(graph, track_labels)\n",
    "print(score_labels, len(score_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa46316",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ()\n",
    "n_set = []\n",
    "\n",
    "track_labels - tarck number\n",
    "for i in track_labels:\n",
    "    graph.nodes[i] -> track_label_i\n",
    "    gather same track_labels_i -> n_set[i]\n",
    "    \n",
    "for cluster in sets:    \n",
    "    build graph from gathered set\n",
    "    run through teh graph to find the biggest edge\n",
    "\n",
    "1) окружность\n",
    "2) посмотреть про центр масс для класстера\n",
    "3) статистика по сумму длин ребер (распредение расстояний)\n",
    "\n",
    "4) https://medium.com/dive-into-ml-ai/faster-implementation-of-mahalanobis-distance-using-tensorflow-42f7aa586bac\n",
    "    scipy.spatial.distance.mahalanobis\n",
    "    \n",
    "5) http://www.open3d.org/docs/release/tutorial/geometry/pointcloud_outlier_removal.html#Radius-outlier-removal\n",
    "    \n",
    "6) https://www.datatechnotes.com/2020/04/anomaly-detection-with-elliptical-envelope-in-python.html\n",
    "    elliptical envilope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
